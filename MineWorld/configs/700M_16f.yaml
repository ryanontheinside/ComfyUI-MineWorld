model:
  target: lvm.LlamaLVM
  params:
    model_class: lvm.LlamaForCausalLM
    tokenizer_config:
      target: vae.VAE
      params:
        config_path: checkpoints/vae/config.json
        ckpt_path: checkpoints/vae/vae.ckpt       
    transformer_config:
      target: transformers.LlamaConfig
      params:
        max_position_embeddings: 5552
        hidden_size: 2048
        intermediate_size: 4096
        num_attention_heads: 32
        num_key_value_heads: 4
        num_hidden_layers: 20
        rope_theta: 10000.0
        torch_dtype: bfloat16
        rms_norm_eps: 1.0e-05
        vocab_size: 8262
        attention_bias: false
        mlp_bias: false